import java.io.File;
import java.io.IOException;

import myutil.Log;
import myutil.ResultList;

import classifier.DiffClassifier;

public class MainDriver {
	/**
	 * 
	 * @param args
	 * @throws IOException 
	 * @throws Exception 
	 */
	public static void main(String[] args) throws IOException {
		String trainArffPath = "/home/elfin/workspace/OngoingProjects/DataMining_Project/data/train.arff";
		String logFilePath = "/home/elfin/workspace/OngoingProjects/DataMining_Project/data/log";
		File logFile = new File(logFilePath);
		if (logFile.exists()) {
			logFile.delete();
		}
		logFile.createNewFile();
		
		MainDriver.runAllClassifiers(logFilePath, trainArffPath);
	}
	
	/**
	 * This is the method to run cross validation on all classifiers to see 
	 * which has the better average performance (lower error rate).
	 * @param logFilePath the path to the log file
	 * @param trainArffPath the path to the training set arff file.
	 */
	public static void runAllClassifiers(String logFilePath, String trainArffPath) {
		double errorRate = 0;
		ResultList results = new ResultList();
		try {
			// 1. C4.5 decision tree
			// 1.1 Laplace smoothing is applied. Vary the pruning confidence:
			Log.addLogToThisPath("### C4.5 smoothing applied, vary pruning confidence:\n", logFilePath);
			double[] pruningConf = {0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50};
			for (Double threshold : pruningConf) {
				errorRate = DiffClassifier.run(new String[] {logFilePath, trainArffPath, 
						"-c45", "-C", threshold.toString(), "-A"});
				Log.addLogToThisPath("*************[-c45 -C " + threshold.toString() + 
						" -A] " + errorRate + " *************\n", logFilePath);
				results.addResult("[-c45 -C " + threshold.toString() + " -A] ", errorRate);
			}
			// 1.2 Laplace smoothing is applied. Use reduced error pruning.
			Log.addLogToThisPath("### C4.5 smoothing applied, use reduced error pruning:\n", logFilePath);
			errorRate = DiffClassifier.run(new String[] {logFilePath, trainArffPath, 
					"-c45", "-R", "-A"}); 
			Log.addLogToThisPath("*************[-c45 -R -A] " + errorRate + " *************\n", logFilePath);
			
			// 1.3 Turn off subtree raising. Laplace smoothing is applied. Vary the pruning confidence:
			Log.addLogToThisPath("### C4.5 smoothing applied, TURN OFF SUBTREE RAISING, " +
					"vary pruning confidence:\n", logFilePath);
			for (Double threshold : pruningConf) {
				errorRate = DiffClassifier.run(new String[] {logFilePath, trainArffPath, 
						"-c45", "-C", threshold.toString(), "-S", "-A"});
				Log.addLogToThisPath("*************[-c45 -C " + threshold.toString() + 
						" -S -A] " + errorRate + " *************\n", logFilePath);
			}
			
			// 1.4 Turn off subtree raising. Laplace smoothing is applied. Use reduced error pruning.
			Log.addLogToThisPath("### C4.5 smoothing applied, TURN OFF SUBTREE RAISING," +
					" use reduced error pruning:\n", logFilePath);
			errorRate = DiffClassifier.run(new String[] {logFilePath, trainArffPath, 
					"-c45", "-R", "-S", "-A"}); 
			Log.addLogToThisPath("*************[-c45 -R -S -A] " + errorRate + " *************\n", logFilePath);
			
			// 2. KNN: with varied K value
			Log.addLogToThisPath("### KNN: Vary K ###\n", logFilePath);
			for (Integer k = 1; k < 21; ++k) {
				errorRate = DiffClassifier.run(new String[] {logFilePath, trainArffPath, 
						"-knn", "-K", k.toString()}); 
				Log.addLogToThisPath("*************[-c45 -K " + k.toString() +
						"] " + errorRate + " *************\n", logFilePath);
			}
			
			// 3. Logistic Regression: varying the ridge estimator
			Log.addLogToThisPath("### Logistic Regression: Default settings ###\n", logFilePath);
			errorRate = DiffClassifier.run(new String[] {logFilePath, 
					trainArffPath, "-logr"}); 
			Log.addLogToThisPath("*************[-logr] " + 
					errorRate + " *************\n", logFilePath);
			
			// 4. Naive Bayes: How to process the numeric features
			// 4.1 Default setting: Guassian Distribution
			Log.addLogToThisPath("### Naive Bayes: Default settings ###\n", logFilePath);
			errorRate = DiffClassifier.run(new String[] {logFilePath, 
					trainArffPath, "-nb"}); 
			Log.addLogToThisPath("*************[-nb] " + 
					errorRate + " *************\n", logFilePath);
			// 4.2 -D: Use supervised discretization to process numeric attributes
			Log.addLogToThisPath("### Naive Bayes: Discretize the numeric features ###\n", logFilePath);
			errorRate = DiffClassifier.run(new String[] {logFilePath, 
					trainArffPath, "-nb", "-D"}); 
			Log.addLogToThisPath("*************[-nb -D] " + 
					errorRate + " *************\n", logFilePath);
			
			// 5. NBTree: No options
			Log.addLogToThisPath("### NaiveBayes Decision Tree ###\n", logFilePath);
			errorRate = DiffClassifier.run(new String[] {logFilePath, 
					trainArffPath, "-nbt"}); 
			Log.addLogToThisPath("*************[-nbt] " + 
					errorRate + " *************\n", logFilePath);
			
			// ================================================================
			// 6. SMO: 
			// 6.1 PolyKernel 1st order:
			Log.addLogToThisPath("### SMO ###\n", logFilePath);
			
			String[] options = weka.core.Utils.splitOptions("-C 1.0 -L 0.0010 -P 1.0E-12 "
					+ "-K \"weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0\"")
			errorRate = DiffClassifier.run(new String[] {logFilePath, 
					trainArffPath, "-smo"}); 
			Log.addLogToThisPath("*************[-nbt] " + 
					errorRate + " *************\n", logFilePath);
			
			
		} catch (Exception e) {
			e.printStackTrace();
			System.exit(1);
		}
	}
	
	/*
	public static void main(String[] args) throws Exception {
		if (args[0].equals("-transtrain")) {
			if (args.length != 3) {
				//TODO: print usage error
				System.exit(1);
			}
			TransTrainArff.main(Arrays.copyOfRange(args, 1, 3));
		}
		else if (args[0].equals("-transtest")) {
			if (args.length != 3) {
				//TODO: print usage error
				System.exit(1);
			}
			TransTestArff.main(Arrays.copyOfRange(args, 1, 3));
		}
		else if (args[0].equals("-diffcls")) {
			DiffClassifier.run(Arrays.copyOfRange(args, 1, args.length));
		}
		else {
			// TODO: print usage error
			System.exit(1);
		}
	}
	*/
}