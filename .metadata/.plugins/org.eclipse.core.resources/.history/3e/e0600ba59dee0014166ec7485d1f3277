package classifier;
import java.util.Random;

import weka.classifiers.Evaluation;
import weka.classifiers.meta.AdaBoostM1;
import weka.classifiers.trees.NBTree;
import weka.core.Instances;
import weka.core.converters.ConverterUtils.DataSource;
public class DiffCls_Ada {	
	/**
	 * 
	 * @param logFilePath the path to the log file
	 * @param trainFilePath the path to the train.arff
	 * @param clsId the identifier for the classifier used in Adaboost
	 * @param clsOptions the options setting of the classifier
	 * @param adaOptions the options setting of Adaboost
	 */
	public static void run(String logFilePath, String trainFilePath, String clsId, 
			String[] clsOptions, String[] adaOptions) {
		DataSource source = new DataSource(trainFilePath);
		Instances data = source.getDataSet();
		data.setClassIndex(data.numAttributes() - 1);
		Random random = new Random(System.currentTimeMillis());

		double sum_of_err = 0;
		// Change the number of iteration for each round.
		for (int i = 0; i < ROUNDS_NUM; ++i) {
			AdaBoostM1 adaboost = new AdaBoostM1();
			NBTree nbtModel = new NBTree();
			adaboost.setClassifier(nbtModel);
			adaboost.setOptions(new String[] {"-S", 
					new Integer(random.nextInt()).toString(),
					// The number of iterations = Current Round Number + 9 !!!
					"-I", new Integer(i + 9).toString()});

			adaboost.buildClassifier(data);
			//System.out.println("Current options of the classifier: " + 
			//		Arrays.toString(adaboost.getOptions()));

			// Evaluation:
			Evaluation eval = new Evaluation(data);
			Instances testData = new DataSource(args[1]).getDataSet();
			testData.setClassIndex(testData.numAttributes() - 1);
			eval.evaluateModel(adaboost, testData);
			System.out.println("Error rate of round " + i + " : " + eval.errorRate());
			sum_of_err += eval.errorRate();
		}
		System.out.println("AVERAGE ERROR RATE: " + sum_of_err/ROUNDS_NUM);
	}
}