How to run the crawler:
1. On the CCIS Linux machine, open the Terminal and cd to current directory.
2. Enter the following command:
       $ python crawler.py
   Then the program should be run in python 2.7 environment and the result of 
   unique URLs crawled without/with the key phrase should be output to two files
   "no_key_result" and "with_key_result".
3. Or if you want to run them seperately, you can enter the command:
       $ python run_no_key.py
   to run the crawler without the key phrase, or enter the command:
       $ python run_with_key.py
   to run the crawler with "concordance".

Results:
The URL results can be found in the folder "Results".
The result I got with key phrase "concordance" is much smaller than my fellows (most of them posted their number of found URLs to be around 500, but mine is:

    The result list length: 149
    Crawled depth: 5
    Total visited pages: 7994
    Ratio of pages containing "concordance": 0.01863897923442582

The reasons for this difference might be:
1. *IMPORTANT* At the last minute before deadline I found that my program only 
   looks for the key phrase within the "data" (i.e. element not inside any tag). 
   A very comman case is the phrase only appears in URLs, but the program still
   considers the page to be invalid because the URL is not in the "data".
2. My program only looks for the key phrase within the <body>...</body>. That
   is to say, it tries to look for the phrase within the page which user can see.
   Thus, it will miss a URL if the phrase only appears in metadata.
3. The program assumes there is no URLs like "//en.wikipedia.org/wiki ...", 
   because from my inspection all these links will be written like "/wiki/...", 
   but maybe there will be exceptions. 
4. My program only looks URLs in the <a> tag.
5. My program will filter out those wikipedia URLs with "&action=edit", because 
   such URLs will direct to the edit page, which is meaningleass.

Author: Yaxin Huang  |  huang.yax@husky.neu.edu
